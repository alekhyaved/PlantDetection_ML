{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plantDataset_tfrecord_creation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltPYJz8_0oXW",
        "outputId": "a5b7c873-0aff-4f69-992c-7ad49b4eb5d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJZWMNDX3Iqq"
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_W45h38_6v"
      },
      "source": [
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1zCPErp9JsR"
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cv2 \n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x0IgySg9MsE",
        "outputId": "910df5d3-ffcd-4ef8-dfe3-0d58fe97b10c"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5JZx0zvCUj5",
        "outputId": "a01ed243-31da-4bbd-90b1-f0ad17a780d4"
      },
      "source": [
        "%cd /gdrive/MyDrive/object_detection/data/"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/object_detection/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsbq_rXwCXRa",
        "outputId": "2661e064-3aaa-4537-8850-7b43851da113"
      },
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAsD8jcrA8O7",
        "outputId": "4a19cb93-34f2-4027-b88c-f41ef92dd23d"
      },
      "source": [
        "for label_path in ['train_labels', 'test_labels']:\n",
        "  print(label_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_labels\n",
            "test_labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BquhBvAXgW",
        "outputId": "7004b4ad-874b-40fb-a516-e359b8b0d2da"
      },
      "source": [
        "\n",
        "#adjusted from: https://github.com/datitran/raccoon_dataset\n",
        "\n",
        "#converts the annotations/labels into one csv file for each training and testing labels\n",
        "#creats label_map.pbtxt file\n",
        "\n",
        "%cd /gdrive/MyDrive/object_detection/data/\n",
        "\n",
        "\n",
        "# images extension\n",
        "images_extension = 'jpg'\n",
        "\n",
        "# takes the path of a directory that contains xml files and converts\n",
        "#  them to one csv file.\n",
        "\n",
        "# returns a csv file that contains: image name, width, height, class, xmin, ymin, xmax, ymax.\n",
        "# note: if the xml file contains more than one box/label, it will create more than one row for the same image. each row contains the info for an individual box. \n",
        "def xml_to_csv(path):\n",
        "  classes_names = []\n",
        "  xml_list = []\n",
        "\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      classes_names.append(member[0].text)\n",
        "      value = (root.find('filename').text ,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text))\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "  classes_names = list(set(classes_names))\n",
        "  classes_names.sort()\n",
        "  return xml_df, classes_names\n",
        "\n",
        "# for both the train_labels and test_labels csv files, it runs the xml_to_csv() above.\n",
        "for label_path in ['train_labels', 'test_labels']:\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\n",
        "  xml_df, classes = xml_to_csv(label_path)\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "# Creating the `label_map.pbtxt` file\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "\n",
        "pbtxt_content = \"\"\n",
        "\n",
        "#creats a pbtxt file the has the class names.\n",
        "for i, class_name in enumerate(classes):\n",
        "    # display_name is optional.\n",
        "    pbtxt_content = (\n",
        "        pbtxt_content\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: 'plants'\\n }}\\n\\n\".format(i + 1, class_name)\n",
        "    )\n",
        "pbtxt_content = pbtxt_content.strip()\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    f.write(pbtxt_content)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/object_detection/data\n",
            "Successfully converted train_labels xml to csv.\n",
            "Successfully converted test_labels xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZV8CLI-DibZ",
        "outputId": "077877c2-092c-402f-d996-5623a67e9259"
      },
      "source": [
        "!cat label_map.pbtxt"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "    id: 1\n",
            "    name: 'plants'\n",
            "    display_name: 'plants'\n",
            " }"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_3eNqu8DqHj",
        "outputId": "d6e19ab9-e8da-4455-c5a6-fabd4ee33e31"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8039\n",
            "drwx------ 2 root root    4096 Nov 19 05:00 'Annotated plant images'\n",
            "-rw------- 1 root root    8188 Nov 20 06:24 'Copy of train_labels.csv'\n",
            "-rw------- 1 root root      65 Nov 20 06:44  label_map.pbtxt\n",
            "-rw------- 1 root root       0 Nov 19 08:38  label_map_sample.pbtxt\n",
            "drwx------ 3 root root    4096 Nov 20 04:16  models\n",
            "drwx------ 2 root root    4096 Nov 20 00:28  plant_images\n",
            "drwx------ 2 root root    4096 Nov 19 05:18  test_labels\n",
            "-rw------- 1 root root    2568 Nov 20 06:44  test_labels.csv\n",
            "-rw------- 1 root root       0 Nov 19 23:55  test_labels.record\n",
            "drwx------ 2 root root    4096 Nov 19 05:18  train_labels\n",
            "-rw------- 1 root root    8188 Nov 20 06:44  train_labels.csv\n",
            "-rw------- 1 root root 8182751 Nov 20 06:42  train_labels.record\n",
            "-rw------- 1 root root    8172 Nov 20 05:27  train_labelsTest.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6aM5v9TCsiC",
        "outputId": "b748b2b6-76d6-4737-bd07-80346eab9afd"
      },
      "source": [
        "#checks if the images box position is placed within the image.\n",
        "\n",
        "#note: while this doesn't checks if the boxes/annotatoins are correctly\n",
        "# placed around the object, Tensorflow will through an error if this occured.\n",
        "%cd /gdrive/MyDrive/object_detection/data/\n",
        "# path to images\n",
        "images_path = 'plant_images'\n",
        "\n",
        "#loops over both train_labels and test_labels csv files to do the check\n",
        "# returns the image name where an error is found \n",
        "# return the incorrect attributes; xmin, ymin, xmax, ymax.\n",
        "for CSV_FILE in ['train_labels.csv', 'test_labels.csv']:\n",
        "  with open(CSV_FILE, 'r') as fid:  \n",
        "      print('[*] Checking file:', CSV_FILE) \n",
        "      file = csv.reader(fid, delimiter=',')\n",
        "      first = True \n",
        "      cnt = 0\n",
        "      error_cnt = 0\n",
        "      error = False\n",
        "      for row in file:\n",
        "          if error == True:\n",
        "              error_cnt += 1\n",
        "              error = False         \n",
        "          if first == True:\n",
        "              first = False\n",
        "              continue     \n",
        "          cnt += 1      \n",
        "          name, width, height, xmin, ymin, xmax, ymax = row[0], int(row[1]), int(row[2]), int(row[4]), int(row[5]), int(row[6]), int(row[7])     \n",
        "          path = os.path.join(images_path, name)\n",
        "          img = cv2.imread(path)         \n",
        "          if type(img) == type(None):\n",
        "              error = True\n",
        "              print('Could not read image', name, img)\n",
        "              continue     \n",
        "          org_height, org_width = img.shape[:2]     \n",
        "          if org_width != width:\n",
        "              error = True\n",
        "              print('Width mismatch for image: ', name, width, '!=', org_width)     \n",
        "          if org_height != height:\n",
        "              error = True\n",
        "              print('Height mismatch for image: ', name, height, '!=', org_height) \n",
        "          if xmin > org_width:\n",
        "              error = True\n",
        "              print('XMIN > org_width for file', name)  \n",
        "          if xmax > org_width:\n",
        "              error = True\n",
        "              print('XMAX > org_width for file', name)\n",
        "          if ymin > org_height:\n",
        "              error = True\n",
        "              print('YMIN > org_height for file', name)\n",
        "          if ymax > org_height:\n",
        "              error = True\n",
        "              print('YMAX > org_height for file', name)\n",
        "          if error == True:\n",
        "              print('Error for file: %s' % name)\n",
        "              print()\n",
        "      print()\n",
        "      print('Checked %d files and realized %d errors' % (cnt, error_cnt))\n",
        "      print(\"-----\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/object_detection/data\n",
            "[*] Checking file: train_labels.csv\n",
            "Could not read image 47.jpg None\n",
            "Could not read image 47.jpg None\n",
            "Could not read image 47.jpg None\n",
            "Could not read image 47.jpg None\n",
            "Could not read image 47.jpg None\n",
            "Could not read image 47.jpg None\n",
            "\n",
            "Checked 208 files and realized 6 errors\n",
            "-----\n",
            "[*] Checking file: test_labels.csv\n",
            "\n",
            "Checked 64 files and realized 0 errors\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITfDVtqJT2bX",
        "outputId": "2aea3f97-7378-4d0e-c39d-4fb5b4d91e1b"
      },
      "source": [
        "import csv\n",
        "print(csv.__file__)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/csv.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nhT8_hLGvjY",
        "outputId": "03d7e945-c62d-4c38-9b70-862eca76ad79"
      },
      "source": [
        "# rm plant_images/'47.jpg'"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'plant_images/47.jpg': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnMUUOIHL79"
      },
      "source": [
        "#removing the entry for it in the csv for that image as well\n",
        "\n",
        "#because we did a random split for the data, we dont know if it ended up being in training or testing\n",
        "# we will remove the image from both.\n",
        "\n",
        "#training\n",
        "#reading the training csv\n",
        "df = pd.read_csv('/gdrive/MyDrive/object_detection/data/train_labels.csv')\n",
        "# removing 47.jpg\n",
        "df = df[df['filename'] != '47.jpg']\n",
        "#reseting the index\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "#saving the df\n",
        "df.to_csv('/gdrive/MyDrive/object_detection/data/train_labels.csv')\n",
        "\n",
        "\n",
        "#testing\n",
        "#reading the testing csv\n",
        "df = pd.read_csv('/gdrive/MyDrive/object_detection/data/test_labels.csv')\n",
        "# removing 47.jpg\n",
        "df = df[df['filename'] != '47.jpg']\n",
        "#reseting the index\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "#saving the df\n",
        "df.to_csv('/gdrive/MyDrive/object_detection/data/test_labels.csv')\n",
        "\n",
        "# Just for the memory\n",
        "df = None\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqKNgrihAYmv"
      },
      "source": [
        "## Don't change the bottom part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDGpdbun9kLR",
        "outputId": "f750279e-edb6-4f51-e6f1-4a8dbafc1989"
      },
      "source": [
        "# #adjusted from: https://github.com/datitran/raccoon_dataset\n",
        "# def xml_to_csv(path):\n",
        "#   classes_names = []\n",
        "#   xml_list = []\n",
        "\n",
        "#   for xml_file in glob.glob(path + '/*.xml'):\n",
        "#     tree = ET.parse(xml_file)\n",
        "#     root = tree.getroot()\n",
        "#     for member in root.findall('object'):\n",
        "#       classes_names.append(member[0].text)\n",
        "#       value = (root.find('filename').text + '.jpg',\n",
        "#                int(root.find('size')[0].text),\n",
        "#                int(root.find('size')[1].text),\n",
        "#                member[0].text,\n",
        "#                int(member[4][0].text),\n",
        "#                int(member[4][1].text),\n",
        "#                int(member[4][2].text),\n",
        "#                int(member[4][3].text))\n",
        "#       xml_list.append(value)\n",
        "#   column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "#   xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "#   classes_names = list(set(classes_names))\n",
        "#   classes_names.sort()\n",
        "#   return xml_df, classes_names\n",
        "\n",
        "# for label_path in ['train_labels', 'test_labels']:\n",
        "#   image_path = os.path.join(os.getcwd(), label_path)\n",
        "#   xml_df, classes = xml_to_csv(label_path)\n",
        "#   xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "#   print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "# label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "# pbtxt_content = \"\"\n",
        "\n",
        "# for i, class_name in enumerate(classes):\n",
        "#     pbtxt_content = (\n",
        "#         pbtxt_content\n",
        "#         + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(i + 1, class_name)\n",
        "#     )\n",
        "# pbtxt_content = pbtxt_content.strip()\n",
        "# with open(label_map_path, \"w\") as f:\n",
        "#     f.write(pbtxt_content)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted train_labels xml to csv.\n",
            "Successfully converted test_labels xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsge68b9-2yn",
        "outputId": "f368ce23-75cb-4af6-b85e-731bbb2adfae"
      },
      "source": [
        "# xml_to_csv('/gdrive/My Drive/object_detection/data/test_labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      filename  width  height   class  xmin  ymin  xmax  ymax\n",
              " 0    1.jpg.jpg    595     890  plants    10    36   595   571\n",
              " 1    5.jpg.jpg   2048    1365  plants  1404   615  2048  1351\n",
              " 2    5.jpg.jpg   2048    1365  plants   119   457   393  1365\n",
              " 3    5.jpg.jpg   2048    1365  plants   778   230  1393  1351\n",
              " 4    5.jpg.jpg   2048    1365  plants  1683    54  1821   621\n",
              " ..         ...    ...     ...     ...   ...   ...   ...   ...\n",
              " 59  11.jpg.jpg   1600    1200  plants     4   314   447   480\n",
              " 60  11.jpg.jpg   1600    1200  plants   883   565   988   642\n",
              " 61  11.jpg.jpg   1600    1200  plants  1039   531  1191   667\n",
              " 62  11.jpg.jpg   1600    1200  plants  1509   565  1598   729\n",
              " 63  11.jpg.jpg   1600    1200  plants  1452   306  1600   431\n",
              " \n",
              " [64 rows x 8 columns], ['plants'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fojFPthcBcu1",
        "outputId": "a076f67f-c72d-43e2-fe78-71338bd73de6"
      },
      "source": [
        "# xml_to_csv('/gdrive/My Drive/object_detection/data/train_labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(       filename  width  height   class  xmin  ymin  xmax  ymax\n",
              " 0    14.jpg.jpg    898    1600  plants    20   322   523   798\n",
              " 1    14.jpg.jpg    898    1600  plants   530   512   898   984\n",
              " 2    13.jpg.jpg   1200    1600  plants     5   632    88   846\n",
              " 3    13.jpg.jpg   1200    1600  plants   942   592  1089   881\n",
              " 4    13.jpg.jpg   1200    1600  plants   470   557   701   971\n",
              " ..          ...    ...     ...     ...   ...   ...   ...   ...\n",
              " 203  49.jpg.jpg    826     683  plants   272   128   351   269\n",
              " 204  49.jpg.jpg    826     683  plants     1     2   214   461\n",
              " 205  49.jpg.jpg    826     683  plants   634   230   826   302\n",
              " 206  55.jpg.jpg    275     183  plants    12    40   150   125\n",
              " 207  55.jpg.jpg    275     183  plants   152     1   275   150\n",
              " \n",
              " [208 rows x 8 columns], ['plants'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7X2ZXJmFH7z",
        "outputId": "49605f89-2da6-4653-d2e8-3569defb468e"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "\n",
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Keras Version: \", tf.keras.__version__)\n",
        "\n",
        "#check GPU\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(physical_devices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-184219fa7e9b>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU Available:  True\n",
            "Tensorflow Version:  2.3.0\n",
            "Keras Version:  2.4.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOPqiuUpFMON",
        "outputId": "51d085ad-b5d2-491b-a0df-55e9254e61e7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 19 23:54:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W /  70W |    227MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-3_4wcuE39T"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMGPt_bkFWAp",
        "outputId": "ae31c43c-5dca-488a-b1c3-af2ae2af4d9c"
      },
      "source": [
        "!protoc --version"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "libprotoc 3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t7NlJtDFz8s",
        "outputId": "cd0d17e1-539e-46ef-eb9a-e47a0dd06b5e"
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /gdrive/My Drive/object_detection/data/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/5a/819537be46d65a01f8b8c6046ed05603fb9ef88c663b8cca840263788d58/avro-python3-1.10.0.tar.gz\n",
            "Collecting apache-beam\n",
            "  Downloading https://files.pythonhosted.org/packages/86/3f/93816e989e8e59b337f22927778494a99b2a3e78a3b6a9e34d043c6fab4e/apache_beam-2.25.0-cp36-cp36m-manylinux2010_x86_64.whl (8.7MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.21)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.1.4)\n",
            "Collecting tf-models-official\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl (840kB)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.11.0)\n",
            "Collecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading https://files.pythonhosted.org/packages/85/a9/473ef678c8862d74c63e11d14afbdbeabe67f92fedd82405de5337d7e6de/fastavro-1.2.0-cp36-cp36m-manylinux2014_x86_64.whl (2.0MB)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.18.5)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.33.2)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->object-detection==0.1) (50.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.6/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.8.3)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.7)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.7.12)\n",
            "Collecting sentencepiece\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.21.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.5.9)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (3.13)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.24.3)\n",
            "Collecting pbr>=0.11\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (4.6)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official->object-detection==0.1) (0.1.5)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.17.2)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (0.0.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.24.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official->object-detection==0.1) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-official->object-detection==0.1) (3.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Building wheels for collected packages: object-detection, avro-python3, dill, hdfs, future, py-cpuinfo\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=1599128 sha256=cc8eab5cf0979c3534992b29c02f628dda08f4bca28466570faa488d001d5e66\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4t_c0dfd/wheels/21/08/f5/4d77e68b51c0bb44926b8748766b236ededc4b2f9631c82aa2\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.0-cp36-none-any.whl size=43735 sha256=44af59f46ac0d74833bdffb80e0d31964a223f3f83261614d786267b4d97065c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/15/cd/fe4ec8b88c130393464703ee8111e2cddebdc40e1b59ea85e9\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=38a54e738a7886163a58f92d99cb7f769384c402cee468146481ea4037a23c33\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for hdfs (setup.py): started\n",
            "  Building wheel for hdfs (setup.py): finished with status 'done'\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=f873935ed2d392b3e03d7f45bb8722bfed81475f4500edf595e8bdfe9d9abbc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=379714e9feb126f7fa6e1ac428ca16df2aa4caba28052599168b064c72032f0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20071 sha256=effe683984bc4034573640ccfa5eb76f37f786c6dd160e083d6c64c1c075d5f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built object-detection avro-python3 dill hdfs future py-cpuinfo\n",
            "Installing collected packages: avro-python3, pyarrow, dill, fastavro, requests, pbr, mock, hdfs, future, apache-beam, tf-slim, lvis, opencv-python-headless, tensorflow-model-optimization, sentencepiece, py-cpuinfo, tf-models-official, object-detection\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed apache-beam-2.25.0 avro-python3-1.10.0 dill-0.3.1.1 fastavro-1.2.0 future-0.18.2 hdfs-2.5.8 lvis-0.5.3 mock-2.0.0 object-detection-0.1 opencv-python-headless-4.4.0.46 pbr-5.5.1 py-cpuinfo-7.0.0 pyarrow-0.17.1 requests-2.25.0 sentencepiece-0.1.94 tensorflow-model-optimization-0.5.0 tf-models-official-2.3.0 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\n",
            "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "ERROR: apache-beam 2.25.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\", but you'll have avro-python3 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLYtwxkuGEHr",
        "outputId": "dbac96aa-4a67-437a-df98-11af5c30744e"
      },
      "source": [
        "!python ./models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-20 06:10:37.757881: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "2020-11-20 06:10:40.416190: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-20 06:10:40.469734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:40.470357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-20 06:10:40.470428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 06:10:40.693502: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-20 06:10:40.846954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-20 06:10:40.862050: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-20 06:10:41.118584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-20 06:10:41.188533: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-20 06:10:41.692240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-20 06:10:41.692485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.693244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.693814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-20 06:10:41.712378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-20 06:10:41.712626: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ce0d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-20 06:10:41.712656: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-20 06:10:41.851947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.852709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ce0bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-20 06:10:41.852740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-20 06:10:41.853741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.854280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-20 06:10:41.854321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 06:10:41.854363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-20 06:10:41.854408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-20 06:10:41.854436: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-20 06:10:41.854462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-20 06:10:41.854483: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-20 06:10:41.854503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-20 06:10:41.854578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.855136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:41.855688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-20 06:10:41.858003: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 06:10:45.622582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-20 06:10:45.622632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-20 06:10:45.622644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-20 06:10:45.626292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:45.627008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 06:10:45.627602: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-20 06:10:45.627653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 10.11s\n",
            "I1120 06:10:50.302237 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 10.11s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I1120 06:10:50.303633 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "I1120 06:10:50.341083 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "I1120 06:10:50.363559 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "I1120 06:10:50.386818 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.15s\n",
            "I1120 06:10:50.542140 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.15s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "I1120 06:10:50.703297 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "I1120 06:10:50.864085 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "I1120 06:10:51.027818 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "I1120 06:10:51.192300 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "I1120 06:10:51.241181 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1120 06:10:51.583290 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1120 06:10:51.583475 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 64\n",
            "I1120 06:10:51.583553 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 3\n",
            "I1120 06:10:51.589960 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:51.616099 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:51.616230 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:51.696624 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:51.696794 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:51.907907 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:51.908066 139929856120704 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 06:10:52.119748 139929856120704 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 06:10:52.119911 139929856120704 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 06:10:52.442445 139929856120704 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 06:10:52.442627 139929856120704 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 06:10:52.766286 139929856120704 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 06:10:52.766478 139929856120704 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 06:10:53.328783 139929856120704 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 06:10:53.328959 139929856120704 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1120 06:10:53.430933 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1120 06:10:53.471305 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:10:53.561493 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1120 06:10:53.561671 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 88\n",
            "I1120 06:10:53.561748 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 4\n",
            "I1120 06:10:53.567706 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:53.590987 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:53.591122 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:53.752043 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:53.752195 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:54.067639 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:54.067829 139929856120704 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 06:10:54.386770 139929856120704 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 06:10:54.386942 139929856120704 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 06:10:54.821746 139929856120704 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 06:10:54.821941 139929856120704 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 06:10:55.250112 139929856120704 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 06:10:55.250401 139929856120704 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 06:10:55.803616 139929856120704 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 06:10:55.803824 139929856120704 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1120 06:10:56.009874 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1120 06:10:56.052328 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:10:56.154118 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1120 06:10:56.154277 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 112\n",
            "I1120 06:10:56.154362 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 5\n",
            "I1120 06:10:56.159819 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:56.183474 139929856120704 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 06:10:56.183594 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:56.519569 139929856120704 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 06:10:56.519753 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:56.838992 139929856120704 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 06:10:56.839159 139929856120704 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 06:10:57.162781 139929856120704 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 06:10:57.162977 139929856120704 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1120 06:10:57.599830 139929856120704 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1120 06:10:57.600003 139929856120704 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1120 06:10:58.042916 139929856120704 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1120 06:10:58.043121 139929856120704 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1120 06:10:58.584264 139929856120704 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1120 06:10:58.584463 139929856120704 efficientnet_model.py:148] round_filter input=320 output=352\n",
            "I1120 06:10:58.796985 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=1408\n",
            "I1120 06:10:58.837872 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:10:58.950377 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1120 06:10:58.950562 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 160\n",
            "I1120 06:10:58.950644 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 6\n",
            "I1120 06:10:58.956480 139929856120704 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1120 06:10:58.980600 139929856120704 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1120 06:10:58.980739 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:10:59.138024 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:10:59.138177 139929856120704 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 06:10:59.461353 139929856120704 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 06:10:59.461560 139929856120704 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 06:10:59.793122 139929856120704 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 06:10:59.793292 139929856120704 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1120 06:11:00.339573 139929856120704 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1120 06:11:00.339743 139929856120704 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1120 06:11:01.104770 139929856120704 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1120 06:11:01.104957 139929856120704 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1120 06:11:01.768218 139929856120704 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1120 06:11:01.768401 139929856120704 efficientnet_model.py:148] round_filter input=320 output=384\n",
            "I1120 06:11:01.980093 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=1536\n",
            "I1120 06:11:02.020720 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:11:02.130230 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1120 06:11:02.130404 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 224\n",
            "I1120 06:11:02.130487 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1120 06:11:02.136506 139929856120704 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 06:11:02.160653 139929856120704 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 06:11:02.160781 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:11:02.321263 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:11:02.321424 139929856120704 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 06:11:02.756552 139929856120704 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 06:11:02.756725 139929856120704 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1120 06:11:03.192126 139929856120704 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1120 06:11:03.192294 139929856120704 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1120 06:11:03.839826 139929856120704 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1120 06:11:03.839997 139929856120704 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1120 06:11:04.490814 139929856120704 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1120 06:11:04.491000 139929856120704 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1120 06:11:05.373137 139929856120704 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1120 06:11:05.373317 139929856120704 efficientnet_model.py:148] round_filter input=320 output=448\n",
            "I1120 06:11:05.582358 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=1792\n",
            "I1120 06:11:05.622100 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:11:05.746233 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1120 06:11:05.746414 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 288\n",
            "I1120 06:11:05.746504 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1120 06:11:05.752464 139929856120704 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 06:11:05.777221 139929856120704 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 06:11:05.777372 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:11:06.310691 139929856120704 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 06:11:06.310900 139929856120704 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 06:11:06.855898 139929856120704 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 06:11:06.856077 139929856120704 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1120 06:11:07.424119 139929856120704 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1120 06:11:07.424309 139929856120704 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1120 06:11:08.222097 139929856120704 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1120 06:11:08.222278 139929856120704 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1120 06:11:08.984735 139929856120704 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1120 06:11:08.984916 139929856120704 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1120 06:11:09.972491 139929856120704 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1120 06:11:09.972670 139929856120704 efficientnet_model.py:148] round_filter input=320 output=512\n",
            "I1120 06:11:10.309549 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=2048\n",
            "I1120 06:11:10.349602 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:11:10.491171 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1120 06:11:10.491343 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1120 06:11:10.491436 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1120 06:11:10.497292 139929856120704 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1120 06:11:10.521585 139929856120704 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1120 06:11:10.521704 139929856120704 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 06:11:10.767594 139929856120704 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 06:11:10.767775 139929856120704 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 06:11:11.450706 139929856120704 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 06:11:11.450914 139929856120704 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1120 06:11:12.132700 139929856120704 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1120 06:11:12.132880 139929856120704 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1120 06:11:13.337544 139929856120704 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1120 06:11:13.337726 139929856120704 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1120 06:11:14.212199 139929856120704 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1120 06:11:14.212401 139929856120704 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1120 06:11:15.416107 139929856120704 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1120 06:11:15.416293 139929856120704 efficientnet_model.py:148] round_filter input=320 output=576\n",
            "I1120 06:11:15.737060 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=2304\n",
            "I1120 06:11:15.776427 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 06:11:15.933497 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1120 06:11:15.933687 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1120 06:11:15.933767 139929856120704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1120 06:11:15.939904 139929856120704 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1120 06:11:15.964885 139929856120704 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1120 06:11:15.965016 139929856120704 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 06:11:16.312824 139929856120704 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 06:11:16.313015 139929856120704 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1120 06:11:17.083301 139929856120704 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1120 06:11:17.083516 139929856120704 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1120 06:11:17.843064 139929856120704 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1120 06:11:17.843238 139929856120704 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1120 06:11:18.944727 139929856120704 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1120 06:11:18.944913 139929856120704 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1120 06:11:20.024563 139929856120704 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1120 06:11:20.024744 139929856120704 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1120 06:11:21.811910 139929856120704 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1120 06:11:21.812100 139929856120704 efficientnet_model.py:148] round_filter input=320 output=640\n",
            "I1120 06:11:22.257404 139929856120704 efficientnet_model.py:148] round_filter input=1280 output=2560\n",
            "I1120 06:11:22.297043 139929856120704 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 31.23s\n",
            "I1120 06:11:22.473137 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 31.23s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I1120 06:11:22.481083 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1120 06:11:22.483353 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1120 06:11:22.484059 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1120 06:11:22.485819 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1120 06:11:22.487681 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1120 06:11:22.488359 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1120 06:11:22.489627 139929856120704 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 42.299s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1PHuRqrGGn5"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2InRpasf5itU",
        "outputId": "8726b477-a991-4d0e-cc4f-410fd532f723"
      },
      "source": [
        "\n",
        "#adjusted from: https://github.com/datitran/raccoon_dataset\n",
        "from object_detection.utils import dataset_util\n",
        "\n",
        "\n",
        "#change this to the base directory where your data/ is \n",
        "data_base_url = '/gdrive/MyDrive/object_detection/data/'\n",
        "\n",
        "#location of images\n",
        "image_dir = data_base_url +'plant_images/'\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "  if row_label == 'plants':\n",
        "    return 1\n",
        "  else:\n",
        "    None\n",
        "\n",
        "def split(df, group):\n",
        "  data = namedtuple('data', ['filename', 'object'])\n",
        "  gb = df.groupby(group)\n",
        "  return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t  encoded_jpg = fid.read()\n",
        "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\timage = Image.open(encoded_jpg_io)\n",
        "\twidth, height = image.size\n",
        "\tfilename = group.filename.encode('utf8')\n",
        "\timage_format = b'jpg'\n",
        "\txmins = []\n",
        "\txmaxs = []\n",
        "\tymins = []\n",
        "\tymaxs = []\n",
        "\tclasses_text = []\n",
        "\tclasses = []\n",
        "\n",
        "\tfor index, row in group.object.iterrows():\n",
        "\t\txmins.append(row['xmin'] / width)\n",
        "\t\txmaxs.append(row['xmax'] / width)\n",
        "\t\tymins.append(row['ymin'] / height)\n",
        "\t\tymaxs.append(row['ymax'] / height)\n",
        "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\tclasses.append(class_text_to_int(row['class']))\n",
        "\n",
        "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t}))\n",
        "\treturn tf_example\n",
        "#creates tfrecord for both csv's\n",
        "# for csv in ['train_labels', 'test_labels']:\n",
        "#     writer = tf.io.TFRecordWriter(data_base_url + csv + '.record')\n",
        "#     path = os.path.join(image_dir)\n",
        "#     examples = pd.read_csv(data_base_url + csv + '.csv')\n",
        "#     grouped = split(examples, 'filename')\n",
        "#     for group in grouped:\n",
        "#       tf_example = create_tf_example(group, path)\n",
        "#       writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "# writer.close()\n",
        "# output_path = os.path.join(os.getcwd(), data_base_url + csv + '.record')\n",
        "# print('Successfully created the TFRecords: {}'.format(data_base_url +csv + '.record'))\n",
        "\n",
        "for csv in ['train_labels', 'test_labels']:\n",
        "  writer = tf.io.TFRecordWriter(data_base_url + csv + '.record')\n",
        "  path = os.path.join(image_dir)\n",
        "  examples = pd.read_csv(data_base_url + csv + '.csv')\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "  writer.close()\n",
        "  output_path = os.path.join(os.getcwd(), data_base_url + csv + '.record')\n",
        "  print('Successfully created the TFRecords: {}'.format(data_base_url +csv + '.record'))\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the TFRecords: /gdrive/MyDrive/object_detection/data/train_labels.record\n",
            "Successfully created the TFRecords: /gdrive/MyDrive/object_detection/data/test_labels.record\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}